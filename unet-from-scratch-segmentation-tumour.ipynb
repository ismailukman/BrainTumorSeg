{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <p style=\"background-color:red;color:white;font-size:22px;text-align:center;border-radius:10px 10px;font-weight:bold;border:2px solid black\">Brain MRI Detector | Segmentation | Using UNet</p>\n\n<center><img src= \"https://www.mayoclinic.org/-/media/kcms/gbs/patient-consumer/images/2014/10/30/15/17/mcdc7_brain_cancer-8col.jpg\" alt =\"Brain-MRI\" style='width:300px;'></center>\n\n**Image source** : Mayo Clinic","metadata":{}},{"cell_type":"markdown","source":"# Quick Index\n\n* [Introduction](#1)\n* [What is Image Segmentation?](#1.1)\n* [EDA](#2)\n* [Image Visualization](#2.1)\n* [Data Generator](#3)\n* [Build UNet](#4)\n    * [UNet Architecture](#4.1)\n* [UNet Implementation](#5)    \n    * [Callbacks](#5.1)\n    * [Performance Metrics](#5.2)\n    * [Model Fit](#5.3)\n    * [Save UNet Model](#5.4)\n* [Evaluation](#6)\n* [Prediction](#7)","metadata":{}},{"cell_type":"markdown","source":"# <span class=\"list-group-item list-group-item-action active\" style=\"color:red;background-color:white;font-size:22px\">Introduction</span> <a id=1></a> \nA brain tumor is a mass or growth of abnormal cells in your brain.Many different types of brain tumors exist. Some brain tumors are noncancerous (benign), and some brain tumors are cancerous (malignant). Brain tumors can begin in your brain (primary brain tumors), or cancer can begin in other parts of your body and spread to your brain as secondary (metastatic) brain tumors.\n\nHow quickly a brain tumor grows can vary greatly. The growth rate as well as the location of a brain tumor determines how it will affect the function of your nervous system.\n\nBrain tumor treatment options depend on the type of brain tumor you have, as well as its size and location. <br>\n**Source credits** : [Mayo Clinic](https://www.mayoclinic.org/diseases-conditions/brain-tumor/symptoms-causes/syc-20350084)","metadata":{}},{"cell_type":"markdown","source":"# <span class=\"list-group-item list-group-item-action active\" style=\"color:red;background-color:white;font-size:22px\">What is Image Segmentation?</span> <a id=1.1></a> <br> \n *  The objective of the Image Segmentation is to classify each pixel of an image with the class it represents, by predicting each pixel in image. <br>\n *  Here in this notebook,we will implement the U-Net model, its a U-shaped architecture (in keras). \nWe will also apply our model to a Brain MRI tumor detection problem to see how it performs.","metadata":{}},{"cell_type":"markdown","source":"# <span class=\"list-group-item list-group-item-action active\" style=\"color:red;background-color:white;font-size:22px\">Import Libraries</span> <a id=1.1.1></a> <br>","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport glob\n\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\nfrom IPython.display import Image\nfrom skimage import io\n\nimport tensorflow as tf\nfrom skimage.color import rgb2gray\nfrom tensorflow.python.keras import Sequential\nfrom tensorflow.keras import layers, optimizers\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.models import Model, load_model, save_model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint,LearningRateScheduler\nfrom tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import plot_model\n\nfrom sklearn.model_selection import train_test_split\nimport cv2\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom IPython.display import display","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-05-28T13:26:13.101177Z","iopub.execute_input":"2023-05-28T13:26:13.101639Z","iopub.status.idle":"2023-05-28T13:26:20.82723Z","shell.execute_reply.started":"2023-05-28T13:26:13.101538Z","shell.execute_reply":"2023-05-28T13:26:20.826109Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_data = pd.read_csv('../input/lgg-mri-segmentation/kaggle_3m/data.csv')\nimg_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:20.832899Z","iopub.execute_input":"2023-05-28T13:26:20.835776Z","iopub.status.idle":"2023-05-28T13:26:20.903603Z","shell.execute_reply.started":"2023-05-28T13:26:20.835733Z","shell.execute_reply":"2023-05-28T13:26:20.902493Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_data.info()","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:20.908376Z","iopub.execute_input":"2023-05-28T13:26:20.910734Z","iopub.status.idle":"2023-05-28T13:26:20.943288Z","shell.execute_reply.started":"2023-05-28T13:26:20.910694Z","shell.execute_reply":"2023-05-28T13:26:20.942081Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:20.947445Z","iopub.execute_input":"2023-05-28T13:26:20.948389Z","iopub.status.idle":"2023-05-28T13:26:20.955444Z","shell.execute_reply.started":"2023-05-28T13:26:20.948351Z","shell.execute_reply":"2023-05-28T13:26:20.954213Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_path = []\nfor sub_dir_path in glob.glob(\"/kaggle/input/lgg-mri-segmentation/kaggle_3m/\"+\"*\"):\n    try:\n        dir_name = sub_dir_path.split('/')[-1]\n        for filename in os.listdir(sub_dir_path):\n            mask_path = sub_dir_path + '/' + filename\n            data_path.extend([dir_name, mask_path])\n    except Exception as e:\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:20.958721Z","iopub.execute_input":"2023-05-28T13:26:20.959569Z","iopub.status.idle":"2023-05-28T13:26:22.77884Z","shell.execute_reply.started":"2023-05-28T13:26:20.959403Z","shell.execute_reply":"2023-05-28T13:26:22.776723Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filenames = data_path[::2]\nmasks = data_path[1::2]","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:22.784652Z","iopub.execute_input":"2023-05-28T13:26:22.78731Z","iopub.status.idle":"2023-05-28T13:26:22.794791Z","shell.execute_reply.started":"2023-05-28T13:26:22.787265Z","shell.execute_reply":"2023-05-28T13:26:22.793537Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame(data={\"patient_id\": filenames,\"img_path\": masks})\ndf","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:22.799878Z","iopub.execute_input":"2023-05-28T13:26:22.80224Z","iopub.status.idle":"2023-05-28T13:26:22.828633Z","shell.execute_reply.started":"2023-05-28T13:26:22.802201Z","shell.execute_reply":"2023-05-28T13:26:22.827737Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"original_img     = df[~df['img_path'].str.contains(\"mask\")]\nmask_img = df[df['img_path'].str.contains(\"mask\")]","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:22.832956Z","iopub.execute_input":"2023-05-28T13:26:22.835319Z","iopub.status.idle":"2023-05-28T13:26:22.85962Z","shell.execute_reply.started":"2023-05-28T13:26:22.83528Z","shell.execute_reply":"2023-05-28T13:26:22.858645Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"original_img,mask_img\n","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:22.864502Z","iopub.execute_input":"2023-05-28T13:26:22.86728Z","iopub.status.idle":"2023-05-28T13:26:22.886385Z","shell.execute_reply.started":"2023-05-28T13:26:22.867242Z","shell.execute_reply":"2023-05-28T13:26:22.885439Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"imgs = sorted(original_img[\"img_path\"].values, key=lambda x : int(x[89:-4]))\nmasks = sorted(mask_img[\"img_path\"].values, key=lambda x : int(x[89:-9]))\n\n# Sorting check\nidx = random.randint(0, len(imgs)-1)\nprint(\"Image path:\", imgs[idx], \"\\nMask path:\", masks[idx])","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:22.894142Z","iopub.execute_input":"2023-05-28T13:26:22.896573Z","iopub.status.idle":"2023-05-28T13:26:22.915159Z","shell.execute_reply.started":"2023-05-28T13:26:22.896533Z","shell.execute_reply":"2023-05-28T13:26:22.913938Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mri_df = pd.DataFrame({\"patient_id\": original_img.patient_id.values,\"img_path\": imgs,\n                           'mask_path':masks})\nmri_df","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:22.919866Z","iopub.execute_input":"2023-05-28T13:26:22.922256Z","iopub.status.idle":"2023-05-28T13:26:22.945786Z","shell.execute_reply.started":"2023-05-28T13:26:22.922217Z","shell.execute_reply":"2023-05-28T13:26:22.944787Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_diagnosis(img_path):\n    value = np.max(cv2.imread(img_path))\n    if value > 0 : \n        return 1\n    else:\n        return 0","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:22.950191Z","iopub.execute_input":"2023-05-28T13:26:22.95268Z","iopub.status.idle":"2023-05-28T13:26:22.960118Z","shell.execute_reply.started":"2023-05-28T13:26:22.952639Z","shell.execute_reply":"2023-05-28T13:26:22.959104Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mri_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:22.964884Z","iopub.execute_input":"2023-05-28T13:26:22.967727Z","iopub.status.idle":"2023-05-28T13:26:22.984675Z","shell.execute_reply.started":"2023-05-28T13:26:22.967688Z","shell.execute_reply":"2023-05-28T13:26:22.98346Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mri_df['mask'] = mri_df['mask_path'].apply(lambda x: get_diagnosis(x))\n\nmri_df['mask_path'] = mri_df['mask_path'].apply(lambda x: str(x))\nmri_df","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:22.989097Z","iopub.execute_input":"2023-05-28T13:26:22.991542Z","iopub.status.idle":"2023-05-28T13:26:47.683273Z","shell.execute_reply.started":"2023-05-28T13:26:22.991493Z","shell.execute_reply":"2023-05-28T13:26:47.682253Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mri_df.drop(columns=['patient_id'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:47.684667Z","iopub.execute_input":"2023-05-28T13:26:47.686343Z","iopub.status.idle":"2023-05-28T13:26:47.692616Z","shell.execute_reply.started":"2023-05-28T13:26:47.686296Z","shell.execute_reply":"2023-05-28T13:26:47.691573Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mri_train, mri_test = train_test_split(mri_df,test_size = 0.1)\nmri_train, mri_val = train_test_split(mri_train,test_size = 0.2)\n\nprint('Training data shape is {0}'.format(mri_train.values.shape))\nprint('Test data shape is {0}'.format(mri_test.values.shape))\nmri_train.sample(n=5)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:47.694153Z","iopub.execute_input":"2023-05-28T13:26:47.694792Z","iopub.status.idle":"2023-05-28T13:26:47.731922Z","shell.execute_reply.started":"2023-05-28T13:26:47.694754Z","shell.execute_reply":"2023-05-28T13:26:47.730981Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <h2 class=\"list-group-item list-group-item-action active\" style=\"color:#FF0000;background-color:white;font-size:25px\">EDA</h2> <a id=2></a> <br>\n\nCheck Balancing in Data","metadata":{}},{"cell_type":"code","source":"mri_df['mask'].value_counts().plot(kind='bar',color=['g','r'],\n                title='Count of Tumour vs No Tumour')","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:47.733827Z","iopub.execute_input":"2023-05-28T13:26:47.734784Z","iopub.status.idle":"2023-05-28T13:26:47.960483Z","shell.execute_reply.started":"2023-05-28T13:26:47.734746Z","shell.execute_reply":"2023-05-28T13:26:47.959796Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mri_df['mask'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:47.962016Z","iopub.execute_input":"2023-05-28T13:26:47.962404Z","iopub.status.idle":"2023-05-28T13:26:47.970925Z","shell.execute_reply.started":"2023-05-28T13:26:47.962365Z","shell.execute_reply":"2023-05-28T13:26:47.969992Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <span class=\"list-group-item list-group-item-action active\" style=\"color:#FF0000;background-color:white;font-size:25px\">Image Visualization</span> <a id=2.1></a> <br>\n\nVisualising the Brain MRI with Tumour","metadata":{}},{"cell_type":"code","source":"count = 0\ni = 0\nfig,axs = plt.subplots(3,3, figsize=(20,15))\nfor mask in mri_df['mask']:\n    if (mask==1):\n        img = io.imread(mri_df.img_path[i])\n        axs[count][0].title.set_text(\"Brain MRI\")\n        axs[count][0].imshow(img)\n        \n        mask = io.imread(mri_df.mask_path[i])\n        axs[count][1].title.set_text(\"Mask =\" + str(mri_df['mask'][i]))\n        axs[count][1].imshow(mask, cmap='gray')\n        \n        img[mask==255] = (255,0,0)  # change pixel color at the position of mask\n        axs[count][2].title.set_text(\"MRI with Mask =\" + str(mri_df['mask'][i]))\n        axs[count][2].imshow(img)\n        count +=1\n    i += 1\n    if (count==3):\n        break\n        \nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:47.972622Z","iopub.execute_input":"2023-05-28T13:26:47.973357Z","iopub.status.idle":"2023-05-28T13:26:49.608368Z","shell.execute_reply.started":"2023-05-28T13:26:47.97332Z","shell.execute_reply":"2023-05-28T13:26:49.607456Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <span class=\"list-group-item list-group-item-action active\" style=\"color:#FF0000;background-color:white;font-size:25px\">Data Generator</span> <a id=3></a> <br>\n### Reference : **[UNet](https://github.com/zhixuhao/unet)**","metadata":{}},{"cell_type":"code","source":"from keras_preprocessing.image import ImageDataGenerator","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:49.609466Z","iopub.execute_input":"2023-05-28T13:26:49.610644Z","iopub.status.idle":"2023-05-28T13:26:49.616386Z","shell.execute_reply.started":"2023-05-28T13:26:49.610612Z","shell.execute_reply":"2023-05-28T13:26:49.615348Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_generator(data_frame, batch_size, aug_dict,\n        image_color_mode=\"rgb\",\n        mask_color_mode=\"grayscale\",\n        image_save_prefix=\"image\",\n        mask_save_prefix=\"mask\",\n        save_to_dir=None,\n        target_size=(256,256),\n        seed=1):\n\n    image_datagen = ImageDataGenerator(**aug_dict)\n    mask_datagen = ImageDataGenerator(**aug_dict)\n    \n    image_generator = image_datagen.flow_from_dataframe(\n        data_frame,\n        x_col = \"img_path\",\n        class_mode = None,\n        color_mode = image_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = image_save_prefix,\n        seed = seed)\n\n    mask_generator = mask_datagen.flow_from_dataframe(\n        data_frame,\n        x_col = \"mask_path\",\n        class_mode = None,\n        color_mode = mask_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = mask_save_prefix,\n        seed = seed)\n\n    train_gen = zip(image_generator, mask_generator)\n    \n    for (img, mask) in train_gen:\n        img, mask = adjust_data(img, mask)\n        yield (img,mask)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:49.61795Z","iopub.execute_input":"2023-05-28T13:26:49.618602Z","iopub.status.idle":"2023-05-28T13:26:49.630779Z","shell.execute_reply.started":"2023-05-28T13:26:49.618563Z","shell.execute_reply":"2023-05-28T13:26:49.629538Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def adjust_data(img,mask):\n    img = img / 255\n    mask = mask / 255\n    mask[mask > 0.5] = 1\n    mask[mask <= 0.5] = 0\n    \n    return (img, mask)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:49.632561Z","iopub.execute_input":"2023-05-28T13:26:49.63336Z","iopub.status.idle":"2023-05-28T13:26:49.642364Z","shell.execute_reply.started":"2023-05-28T13:26:49.633321Z","shell.execute_reply":"2023-05-28T13:26:49.641319Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_generator_args = dict(rotation_range=0.2,\n                            width_shift_range=0.05,\n                            height_shift_range=0.05,\n                            shear_range=0.05,\n                            zoom_range=0.05,\n                            horizontal_flip=True,\n                            fill_mode='nearest')\n\ntrain_gen = train_generator(mri_train, 32,\n                                train_generator_args,\n                                target_size=(256, 256))\n    \nval_gen = train_generator(mri_val, 32,\n                                dict(),\n                                target_size=(256, 256))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:49.644122Z","iopub.execute_input":"2023-05-28T13:26:49.644822Z","iopub.status.idle":"2023-05-28T13:26:49.655494Z","shell.execute_reply.started":"2023-05-28T13:26:49.644784Z","shell.execute_reply":"2023-05-28T13:26:49.654567Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"color:#FF0000;background-color:white;font-size:25px\">Build UNet Model</span> <a id=4></a> <br>\n\n# <span class=\"list-group-item list-group-item-action active\" style=\"color:#FF0000;background-color:white;font-size:25px\">UNet Architecture</span> <a id=4.1></a>\n\n<center><img src= \"https://miro.medium.com/max/1200/1*f7YOaE4TWubwaFF7Z1fzNw.png\" alt =\"UNet\" style='width:800px;'></center>","metadata":{}},{"cell_type":"markdown","source":"UNet is named after the shape of it's architecture (U - shaped). UNet model is used to solve Image Segmentation problems especially in Medical related problems.\n\nThe input layer has 572x572x1 dimension in the above architecture. The 1 column dimension specifies the input is b/w image. If the input dimension was 572x572x3 , the input image is rgb coloured image.\nAfter the input layer,we can divide this UNet architecture into 4 parts\n* Encoder Part (Contracting path)\n* Upsampling2D\n* Decoder part (Expanding Path)\n* Skip Connection (Residual connection)\n\n\nBriefly into the Details of these parts: <br>\n**Encoder Part**\nThe initial half of the U-shaped architecture is the Contracting path(Encoder part). As the name Contracting path denotes, the dimension of the input image keeps reducing as it passes through 2 Conv2D 3X3 kernel size, ReLU activation and filters=64 followed by Maxpool2D layer (reduces size divided by 2). Now again 2 - Conv2D layers (128 filters) and 1 Maxpooling2D , 2 - Conv2D layers (256 filters) and 1 Maxpooling2D completely contracts the output dimension through layers. This is similar to the Zoom in of the image through each layer. \n\n**Upsampling2D**\nThe Upsampling2D layer upsamples the layer output dimension by duplicating the row values twice.The concatenation of the upsampling2D and contracting path happens here leading into the Expanding path.\nThe Upsampling layer is present after 2 conv2D layers in Decoder part, similar to presence of  Maxpooling as in Encoder part.\n\n**Decoder Path**\nThe second half of the U-shaped architecture is the Expanding path(Decoder part). As the name Expanding path denotes, the dimension of the image keeps expanding as it passes through 2 Conv2D 3X3 kernel size, ReLU activation and filters=512 followed by Upsampling2D layer (duplicates size divided by 2). Now again 2 - Conv2D layers (256 filters) and 1 Upsampling2D ,2 - Conv2D layers (128 filters) and 1 Upsampling2D, 2 - Conv2D layers (64 filters) completely expands the output dimension through layers. This is similar to the Zoom in of the image through each layer. \n\n**Skip Connection (Residual connection)**\nThe Skip Connection (Residual connection) alias the identity mapping is present after each of the 2 Conv2D layers in Encoder to the corresponding same shaped dimension layer in the Decoder part . In the above architecture the copy and crop does residual connection task.The Concatenate layer does the work of adding those two layers. This skip connection doesnot harm the model in any way even in worst case but definitely benificial to the output in the model.\n\nFinally the decoder part ends with the Output Segmentation map with filters 2. This layer is again passed through a Conv2D with Sigmoid activation and filters 1. The output classifies each pixel if tumour is present or not in it. \n\n**UNet Model build**\nHere in the model built below, after each of second Convolutional layer,Batch Normalization layer with axis=3 is added to prevent \"internal covariance shift\" , the Activation layer \"relu\" is added here instead of that second Convolutional layer.\n\n### Reference : [UNet](https://github.com/zhixuhao/unet)","metadata":{}},{"cell_type":"code","source":"def unet(pretrained_weights = None,input_size = (256,256,3)):\n    inputs = Input(input_size)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n    conv1 = Conv2D(64, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n    bn_1 = BatchNormalization(axis=3)(conv1)\n    act_1 = Activation('relu')(bn_1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(act_1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n    conv2 = Conv2D(128, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n    bn_2 = BatchNormalization(axis=3)(conv2)\n    act_2 = Activation('relu')(bn_2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(act_2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n    conv3 = Conv2D(256, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv3)\n    bn_3 = BatchNormalization(axis=3)(conv3)\n    act_3 = Activation('relu')(bn_3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(act_3)\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n    conv4 = Conv2D(512, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv4)\n    bn_4 = BatchNormalization(axis=3)(conv4)\n    act_4 = Activation('relu')(bn_4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(act_4)\n\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n    conv5 = Conv2D(1024, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n    bn_5 = BatchNormalization(axis=3)(conv5)\n    act_5 = Activation('relu')(bn_5)\n\n    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(act_5))\n    merge6 = concatenate([conv4,up6], axis = 3)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n    conv6 = Conv2D(512, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv6)\n    bn_6 = BatchNormalization(axis=3)(conv6)\n    act_6 = Activation('relu')(bn_6)\n    \n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(act_6))\n    merge7 = concatenate([conv3,up7], axis = 3)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n    conv7 = Conv2D(256, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv7)\n    bn_7 = BatchNormalization(axis=3)(conv7)\n    act_7 = Activation('relu')(bn_7)\n\n    \n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(act_7))\n    merge8 = concatenate([conv2,up8], axis = 3)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n    conv8 = Conv2D(128, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv8)\n    bn_8 = BatchNormalization(axis=3)(conv8)\n    act_8 = Activation('relu')(bn_8)\n    \n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(act_8))\n    merge9 = concatenate([conv1,up9], axis = 3)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    conv9 = Conv2D(2, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    bn_9 = BatchNormalization(axis=3)(conv9)\n    act_9 = Activation('relu')(bn_9)\n    \n    conv10 = Conv2D(1, 1, activation = 'sigmoid')(act_9)\n\n    model = model = Model(inputs=[inputs] , outputs = [conv10])    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:49.658718Z","iopub.execute_input":"2023-05-28T13:26:49.659015Z","iopub.status.idle":"2023-05-28T13:26:49.686977Z","shell.execute_reply.started":"2023-05-28T13:26:49.658988Z","shell.execute_reply":"2023-05-28T13:26:49.68603Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = unet()\nmodel.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-05-28T13:26:49.688417Z","iopub.execute_input":"2023-05-28T13:26:49.68888Z","iopub.status.idle":"2023-05-28T13:26:52.869139Z","shell.execute_reply.started":"2023-05-28T13:26:49.688818Z","shell.execute_reply":"2023-05-28T13:26:52.86811Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <span class=\"list-group-item list-group-item-action active\" style=\"color:#FF0000;background-color:white;font-size:25px\">UNet Model architecture</span> <a id=5></a>\n","metadata":{}},{"cell_type":"code","source":"plot_model(model, to_file='unet_model.png', show_shapes=True)\nImage(\"unet_model.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-05-28T13:26:52.870719Z","iopub.execute_input":"2023-05-28T13:26:52.871369Z","iopub.status.idle":"2023-05-28T13:26:54.293631Z","shell.execute_reply.started":"2023-05-28T13:26:52.871327Z","shell.execute_reply":"2023-05-28T13:26:54.289778Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <span class=\"list-group-item list-group-item-action active\" style=\"color:#FF0000;background-color:white;font-size:25px\">Callbacks</span> <a id=5.1></a> <br>\n\nThe Callbacks used are \n* Learning Rate reduction\n* Early Stopping\n* Model Checkpoint","metadata":{}},{"cell_type":"code","source":"lr_reduction = ReduceLROnPlateau(monitor='val_iou',patience=8, verbose=1,\n                                 factor=0.4, min_lr=0.0001)\n\nearly_stop = EarlyStopping(monitor='val_iou', min_delta=0.00001, patience=6, \n                           mode='auto', restore_best_weights=True)\n\nmodel_checkpoint = ModelCheckpoint('unet_membrane.hdf5', verbose=1,monitor='val_iou',\n                                   save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:54.295607Z","iopub.execute_input":"2023-05-28T13:26:54.296854Z","iopub.status.idle":"2023-05-28T13:26:54.305667Z","shell.execute_reply.started":"2023-05-28T13:26:54.296804Z","shell.execute_reply":"2023-05-28T13:26:54.304558Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <span class=\"list-group-item list-group-item-action active\" style=\"color:#FF0000;background-color:white;font-size:25px\">Performance Metrics</span> <a id=5.2></a>\n","metadata":{}},{"cell_type":"code","source":"smooth =100\ndef iou(y_true, y_pred):\n    intersection = K.sum(y_true * y_pred)\n    sum_ = K.sum(y_true + y_pred)\n    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n    return jac","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:54.313869Z","iopub.execute_input":"2023-05-28T13:26:54.314733Z","iopub.status.idle":"2023-05-28T13:26:54.34542Z","shell.execute_reply.started":"2023-05-28T13:26:54.314679Z","shell.execute_reply":"2023-05-28T13:26:54.344364Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_coef(y_true, y_pred):\n    y_truef=K.flatten(y_true)\n    y_predf=K.flatten(y_pred)\n    And=K.sum(y_truef* y_predf)\n    return((2* And + smooth) / (K.sum(y_truef) + K.sum(y_predf) + smooth))\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1-dice_coef(y_true, y_pred)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-05-28T13:26:54.348952Z","iopub.execute_input":"2023-05-28T13:26:54.349652Z","iopub.status.idle":"2023-05-28T13:26:54.356753Z","shell.execute_reply.started":"2023-05-28T13:26:54.349614Z","shell.execute_reply":"2023-05-28T13:26:54.355763Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <span class=\"list-group-item list-group-item-action active\" style=\"color:#FF0000;background-color:white;font-size:25px\">Model Fit</span> <a id=5.3></a>\n","metadata":{}},{"cell_type":"code","source":"EPOCH = 100\nlearning_rate = 1e-3","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:54.358425Z","iopub.execute_input":"2023-05-28T13:26:54.358886Z","iopub.status.idle":"2023-05-28T13:26:54.367251Z","shell.execute_reply.started":"2023-05-28T13:26:54.358849Z","shell.execute_reply":"2023-05-28T13:26:54.366232Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=None, decay=1e-3/32, amsgrad=False), \n              loss=dice_coef_loss, metrics=[\"binary_accuracy\", iou, dice_coef])","metadata":{"execution":{"iopub.status.busy":"2023-05-28T13:26:54.368832Z","iopub.execute_input":"2023-05-28T13:26:54.369314Z","iopub.status.idle":"2023-05-28T13:26:54.392285Z","shell.execute_reply.started":"2023-05-28T13:26:54.369278Z","shell.execute_reply":"2023-05-28T13:26:54.391186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(train_gen,steps_per_epoch=len(mri_train)/32,\n                    epochs=EPOCH ,validation_data=val_gen,validation_steps=len(mri_val) / 32,\n                    callbacks=[model_checkpoint])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-05-28T13:26:54.394979Z","iopub.execute_input":"2023-05-28T13:26:54.395685Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <span class=\"list-group-item list-group-item-action active\" style=\"color:#FF0000;background-color:white;font-size:25px\">Save Model</span> <a id=5.4></a>\n","metadata":{}},{"cell_type":"code","source":"!mkdir -p saved_model\nmodel.save('saved_model/my_model.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# saving model achitecture in json file\nmodel_json = model.to_json()\nwith open(\"UNet-seg-model.json\", \"w\") as json_file:\n    json_file.write(model_json)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <span class=\"list-group-item list-group-item-action active\" style=\"color:#FF0000;background-color:white;font-size:25px\">Evaluation</span> <a id=6></a>\n\nEvaluation metrics are listed below","metadata":{}},{"cell_type":"code","source":"history.history.keys()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Loss function** :  IOU - Intersection over Union","metadata":{}},{"cell_type":"markdown","source":"![Performance metric : Intersection over Union](data:image/png;base64,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)\n\n![IOU](data:image/png;base64,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)","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.plot(history.history['iou']);\nplt.plot(history.history['val_iou']);\nplt.title(\"Segmentation Model IOU Loss\");\nplt.ylabel(\"IOU loss\");\nplt.xlabel(\"Epochs\");\nplt.legend(['train', 'val']);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <span class=\"list-group-item list-group-item-action active\" style=\"color:#FF0000;background-color:white;font-size:25px\">Prediction</span> <a id=7></a>\n","metadata":{}},{"cell_type":"code","source":"test_ids = list(mri_test.img_path)\ntest_mask = list(mri_test.mask_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prediction(test, model_seg):\n  \n    # empty list to store results\n    mask, image_id,has_mask = [], [], []\n    \n    #itetrating through each image in test data\n    for i in test.img_path:\n        \n\n        \n        #Creating a empty array of shape 1,256,256,1\n        X = np.empty((1,256,256,3))\n        # read the image\n        img = io.imread(i)\n        #resizing the image and coverting them to array of type float64\n        img = cv2.resize(img, (256,256))\n        img = np.array(img, dtype=np.float64)\n        \n        # standardising the image\n        img -= img.mean()\n        img /= img.std()\n        #converting the shape of image from 256,256,3 to 1,256,256,3\n        X[0,] = img\n        \n        #make prediction of mask\n        predict = model_seg.predict(X)\n        \n        # if sum of predicted mask is 0 then there is not tumour\n        if predict.round().astype(int).sum()==0:\n            image_id.append(i)\n            has_mask.append(0)\n            mask.append('No mask :)')\n        else:\n        #if the sum of pixel values are more than 0, then there is tumour\n            image_id.append(i)\n            has_mask.append(1)\n            mask.append(predict)\n            \n    return pd.DataFrame({'img_path': image_id,'predicted_mask': mask,'has_mask': has_mask})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_pred = prediction(mri_test, model)\ndf_pred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_pred = mri_test.merge(df_pred, on='img_path')\ndf_pred.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"count = 0\nfig, axs = plt.subplots(3,5, figsize=(30,15))\n\nfor i in range(len(df_pred)):\n    if df_pred.has_mask[i]==1 and count<15:\n        #read mri images\n        img = io.imread(df_pred.img_path[i])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        axs[count][0].imshow(img)\n        axs[count][0].title.set_text('Brain MRI')\n        \n        #read original mask\n        mask = io.imread(df_pred.mask_path[i])\n        axs[count][1].imshow(mask)\n        axs[count][1].title.set_text('Ground Truth')\n        \n        #read predicted mask\n        pred = np.array(df_pred.predicted_mask[i]).squeeze().round()\n        axs[count][2].imshow(pred)\n        axs[count][2].title.set_text('predicted mask')\n        \n        #overlay original mask with MRI\n        img[mask==255] = (255,0,0)\n        axs[count][3].imshow(img)\n        axs[count][3].title.set_text('Brain MRI with original mask (Ground Truth)')\n        \n        #overlay predicted mask and MRI\n        img_ = io.imread(df_pred.img_path[i])\n        img_ = cv2.cvtColor(img_, cv2.COLOR_BGR2RGB)\n        img_[pred==1] = (0,255,150)\n        axs[count][4].imshow(img_)\n        axs[count][4].title.set_text('MRI with PREDICTED MASK')\n        \n        count +=1\n    if (count==3):\n        break\n\nfig.tight_layout()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Reference:** \n* [UNet model Original creator Reference link](https://github.com/zhixuhao/unet)\n* [Kaggle Notebook - SAMUEL CORTINHAS](https://www.kaggle.com/code/samuelcortinhas/case-study-u-net-from-scratch#Application:-Tumor-detection)","metadata":{}}]}