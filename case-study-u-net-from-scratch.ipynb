{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we will implement (a variant of) the **U-Net** model, name after its U-shaped architecture, from scratch in **keras**. It is a popular framework for image **segmentation** tasks, with applications in medical imaging and self-driving cars, etc. \n\nWe will also apply our model to a Brain MRI **tumor detection** problem to see how it performs.","metadata":{}},{"cell_type":"markdown","source":"**Architecture**","metadata":{}},{"cell_type":"markdown","source":"![unet](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)","metadata":{}},{"cell_type":"markdown","source":"The noteable features of the network include many **convolution + relu** and **max pooling** layers in the encoder (first half) to contextualise the inputs. The decoder (second half) on the other hand makes use of **transpose convolutions** to increase the images sizes back to that of the input (since the input and output sizes have to match). \n\nThe drawback to this is a loss of **spatial information** due to the shrinking image size. To overcome this, the model uses **skip connections** to relay spatial information from the encoder to the decoder after every block of convolutions. This results in a high resolution and high accuracy segmentation method whilst keeping the computational cost relatively low.","metadata":{}},{"cell_type":"markdown","source":"**Libraries**","metadata":{}},{"cell_type":"code","source":"# Core\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport gc\nimport os\nimport cv2\nfrom tqdm import tqdm_notebook, tnrange\nfrom glob import glob\nfrom itertools import chain\nfrom skimage.color import rgb2gray\nfrom skimage.io import imread, imshow, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\n# Tensorflow\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom keras.utils.vis_utils import plot_model","metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Reproducibility**","metadata":{}},{"cell_type":"code","source":"# Set random seeds\ndef set_seed(seed=0):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nset_seed()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:42:42.853753Z","iopub.execute_input":"2022-06-13T13:42:42.854159Z","iopub.status.idle":"2022-06-13T13:42:42.878248Z","shell.execute_reply.started":"2022-06-13T13:42:42.854129Z","shell.execute_reply":"2022-06-13T13:42:42.876319Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# U-Net","metadata":{}},{"cell_type":"markdown","source":"You can find the original paper [here](https://arxiv.org/pdf/1505.04597.pdf) (from 2015).","metadata":{}},{"cell_type":"markdown","source":"**Encoder**","metadata":{}},{"cell_type":"code","source":"def conv_block(inputs=None, n_filters=64, max_pooling=True):\n    \"\"\"\n    Convolutional downsampling block\n    \n    Arguments:\n        inputs -- Input tensor\n        n_filters -- Number of filters for the convolutional layers\n        max_pooling -- Use MaxPooling2D to reduce the spatial dimensions of the output volume\n    Returns: \n        next_layer, skip_connection --  Next layer and skip connection outputs\n    \"\"\"\n    \n    # Convolutional layers\n    conv = layers.Conv2D(n_filters,\n                  kernel_size = (3, 3),\n                  activation = 'relu',\n                  padding = 'same',\n                  kernel_initializer = 'he_normal')(inputs)\n    conv = layers.Conv2D(n_filters,\n                  kernel_size = (3, 3),\n                  activation = None,\n                  padding = 'same',\n                  kernel_initializer = 'he_normal')(conv)\n    skip_connection = conv\n    conv = layers.BatchNormalization(axis=3)(conv)\n    conv = layers.Activation('relu')(conv)\n        \n    # if max_pooling is True add a MaxPooling2D with 2x2 pool_size\n    if max_pooling:\n        next_layer = layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2))(conv)\n    else:\n        next_layer = conv\n    \n    return next_layer, skip_connection","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:42:42.930866Z","iopub.execute_input":"2022-06-13T13:42:42.931176Z","iopub.status.idle":"2022-06-13T13:42:42.938641Z","shell.execute_reply.started":"2022-06-13T13:42:42.931151Z","shell.execute_reply":"2022-06-13T13:42:42.937789Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Decoder**","metadata":{}},{"cell_type":"code","source":"def upsampling_block(expansive_input, contractive_input, n_filters=64):\n    \"\"\"\n    Convolutional upsampling block\n    \n    Arguments:\n        expansive_input -- Input tensor from previous layer\n        contractive_input -- Input tensor from previous skip layer\n        n_filters -- Number of filters for the convolutional layers\n    Returns: \n        conv -- Tensor output\n    \"\"\"\n    \n    # Transpose convolution\n    up = layers.Conv2DTranspose(\n                 n_filters,\n                 kernel_size = (2, 2),\n                 strides = (2, 2),\n                 padding = 'same')(expansive_input)\n    \n    # Merge the previous output and the contractive_input\n    merge = layers.concatenate([up, contractive_input], axis=3)\n    conv = layers.Conv2D(n_filters,\n                 kernel_size = (3, 3),\n                 activation = 'relu',\n                 padding = 'same',\n                 kernel_initializer = 'he_normal')(merge)\n    conv = layers.Conv2D(n_filters,\n                 kernel_size = (3, 3),\n                 activation = None,\n                 padding = 'same',\n                 kernel_initializer = 'he_normal')(conv)\n    conv = layers.BatchNormalization(axis=3)(conv)\n    conv = layers.Activation('relu')(conv)\n    \n    return conv","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:42:43.014737Z","iopub.execute_input":"2022-06-13T13:42:43.015075Z","iopub.status.idle":"2022-06-13T13:42:43.025582Z","shell.execute_reply.started":"2022-06-13T13:42:43.015048Z","shell.execute_reply":"2022-06-13T13:42:43.024655Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model**","metadata":{}},{"cell_type":"code","source":"# 23 layer CNN\ndef build_unet(input_size=(256, 256, 3), n_filters=64, n_classes=1):\n    \"\"\"\n    Unet model\n    \n    Arguments:\n        input_size -- Input shape \n        n_filters -- Number of filters for the convolutional layers\n        n_classes -- Number of output classes\n    Returns: \n        model -- tf.keras.Model\n    \"\"\"\n    \n    # Input layer\n    inputs = layers.Input(input_size)\n    \n    # Encoder (double the number of filters at each step)\n    cblock1 = conv_block(inputs, n_filters)\n    cblock2 = conv_block(cblock1[0], 2*n_filters)\n    cblock3 = conv_block(cblock2[0], 4*n_filters)\n    cblock4 = conv_block(cblock3[0], 8*n_filters)\n    cblock5 = conv_block(cblock4[0], 16*n_filters, max_pooling=False) \n    \n    # Decoder (halve the number of filters at each step)\n    ublock6 = upsampling_block(cblock5[0], cblock4[1],  8*n_filters)\n    ublock7 = upsampling_block(ublock6, cblock3[1],  4*n_filters)\n    ublock8 = upsampling_block(ublock7, cblock2[1],  2*n_filters)\n    ublock9 = upsampling_block(ublock8, cblock1[1],  n_filters)\n\n    # 1x1 convolution\n    conv10 = layers.Conv2D(filters = n_classes,\n                 kernel_size = (1, 1),\n                 activation = 'sigmoid',    # use softmax if n_classes>1\n                 padding = 'same')(ublock9)\n\n    model = keras.Model(inputs=inputs, outputs=conv10)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:42:43.090919Z","iopub.execute_input":"2022-06-13T13:42:43.091226Z","iopub.status.idle":"2022-06-13T13:42:43.100295Z","shell.execute_reply.started":"2022-06-13T13:42:43.091198Z","shell.execute_reply":"2022-06-13T13:42:43.099532Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Alternative implementation**","metadata":{}},{"cell_type":"code","source":"# Long form\ndef build_unet_alt(input_size=(256, 256, 3), n_filters=64, n_classes=1):\n    \"\"\"\n    Unet model\n    \n    Arguments:\n        input_size -- Input shape \n        n_filters -- Number of filters for the convolutional layers\n        n_classes -- Number of output classes\n    Returns: \n        model -- tf.keras.Model\n    \"\"\"\n    \n    # Input layer\n    inputs = layers.Input(input_size)\n    \n    # Block 1 (encoder)\n    conv1 = layers.Conv2D(n_filters,\n                  kernel_size = (3, 3),\n                  activation = 'relu',\n                  padding = 'same',\n                  kernel_initializer = 'he_normal')(inputs)\n    conv1 = layers.Conv2D(n_filters,\n                  kernel_size = (3, 3),\n                  activation = None,\n                  padding = 'same',\n                  kernel_initializer = 'he_normal')(conv1)\n    bn1 = layers.BatchNormalization(axis=3)(conv1)\n    act1 = layers.Activation('relu')(bn1)\n    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(act1)\n    \n    # Block 2 (encoder)\n    conv2 = layers.Conv2D(2*n_filters,\n                  kernel_size = (3, 3),\n                  activation = 'relu',\n                  padding = 'same',\n                  kernel_initializer = 'he_normal')(pool1)\n    conv2 = layers.Conv2D(2*n_filters,\n                  kernel_size = (3, 3),\n                  activation = None,\n                  padding = 'same',\n                  kernel_initializer = 'he_normal')(conv2)\n    bn2 = layers.BatchNormalization(axis=3)(conv2)\n    act2 = layers.Activation('relu')(bn2)\n    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(act2)\n    \n    # Block 3 (encoder)\n    conv3 = layers.Conv2D(4*n_filters,\n                  kernel_size = (3, 3),\n                  activation = 'relu',\n                  padding = 'same',\n                  kernel_initializer = 'he_normal')(pool2)\n    conv3 = layers.Conv2D(4*n_filters,\n                  kernel_size = (3, 3),\n                  activation = None,\n                  padding = 'same',\n                  kernel_initializer = 'he_normal')(conv3)\n    bn3 = layers.BatchNormalization(axis=3)(conv3)\n    act3 = layers.Activation('relu')(bn3)\n    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(act3)\n    \n    # Block 4 (encoder)\n    conv4 = layers.Conv2D(8*n_filters,\n                  kernel_size = (3, 3),\n                  activation = 'relu',\n                  padding = 'same',\n                  kernel_initializer = 'he_normal')(pool3)\n    conv4 = layers.Conv2D(8*n_filters,\n                  kernel_size = (3, 3),\n                  activation = None,\n                  padding = 'same',\n                  kernel_initializer = 'he_normal')(conv4)\n    bn4 = layers.BatchNormalization(axis=3)(conv4)\n    act4 = layers.Activation('relu')(bn4)\n    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(act4)\n    \n    # Block 5 (bottleneck)\n    conv5 = layers.Conv2D(16*n_filters,\n                  kernel_size = (3, 3),\n                  activation = 'relu',\n                  padding = 'same',\n                  kernel_initializer = 'he_normal')(pool4)\n    conv5 = layers.Conv2D(16*n_filters,\n                  kernel_size = (3, 3),\n                  activation = None,\n                  padding = 'same',\n                  kernel_initializer = 'he_normal')(conv5)\n    bn5 = layers.BatchNormalization(axis=3)(conv5)\n    act5 = layers.Activation('relu')(bn5)\n    \n    # Block 6 (decoder)\n    up6 = layers.Conv2DTranspose(\n                 8*n_filters,\n                 kernel_size = (2, 2),\n                 strides = (2, 2),\n                 padding = 'same')(act5)\n    merge6 = layers.concatenate([up6, conv4], axis=3)\n    conv6 = layers.Conv2D(8*n_filters,\n                 kernel_size = (3, 3),\n                 activation = 'relu',\n                 padding = 'same',\n                 kernel_initializer = 'he_normal')(merge6)\n    conv6 = layers.Conv2D(8*n_filters,\n                 kernel_size = (3, 3),\n                 activation = None,\n                 padding = 'same',\n                 kernel_initializer = 'he_normal')(conv6)\n    bn6 = layers.BatchNormalization(axis=3)(conv6)\n    act6 = layers.Activation('relu')(bn6)\n    \n    # Block 7 (decoder)\n    up7 = layers.Conv2DTranspose(\n                 4*n_filters,\n                 kernel_size = (2, 2),\n                 strides = (2, 2),\n                 padding = 'same')(act6)\n    merge7 = layers.concatenate([up7, conv3], axis=3)\n    conv7 = layers.Conv2D(4*n_filters,\n                 kernel_size = (3, 3),\n                 activation = 'relu',\n                 padding = 'same',\n                 kernel_initializer = 'he_normal')(merge7)\n    conv7 = layers.Conv2D(4*n_filters,\n                 kernel_size = (3, 3),\n                 activation = None,\n                 padding = 'same',\n                 kernel_initializer = 'he_normal')(conv7)\n    bn7 = layers.BatchNormalization(axis=3)(conv7)\n    act7 = layers.Activation('relu')(bn7)\n    \n    # Block 8 (decoder)\n    up8 = layers.Conv2DTranspose(\n                 2*n_filters,\n                 kernel_size = (2, 2),\n                 strides = (2, 2),\n                 padding = 'same')(act7)\n    merge8 = layers.concatenate([up8, conv2], axis=3)\n    conv8 = layers.Conv2D(2*n_filters,\n                 kernel_size = (3, 3),\n                 activation = 'relu',\n                 padding = 'same',\n                 kernel_initializer = 'he_normal')(merge8)\n    conv8 = layers.Conv2D(2*n_filters,\n                 kernel_size = (3, 3),\n                 activation = None,\n                 padding = 'same',\n                 kernel_initializer = 'he_normal')(conv8)\n    bn8 = layers.BatchNormalization(axis=3)(conv8)\n    act8 = layers.Activation('relu')(bn8)\n    \n    # Block 9 (decoder)\n    up9 = layers.Conv2DTranspose(\n                 n_filters,\n                 kernel_size = (2, 2),\n                 strides = (2, 2),\n                 padding = 'same')(act8)\n    merge9 = layers.concatenate([up9, conv1], axis=3)\n    conv9 = layers.Conv2D(n_filters,\n                 kernel_size = (3, 3),\n                 activation = 'relu',\n                 padding = 'same',\n                 kernel_initializer = 'he_normal')(merge9)\n    conv9 = layers.Conv2D(n_filters,\n                 kernel_size = (3, 3),\n                 activation = None,\n                 padding = 'same',\n                 kernel_initializer = 'he_normal')(conv9)\n    bn9 = layers.BatchNormalization(axis=3)(conv9)\n    act9 = layers.Activation('relu')(bn9)\n    \n    # Output layer (1x1 convolution)\n    conv10 = layers.Conv2D(filters = n_classes,\n                 kernel_size = (1, 1),\n                 activation = 'sigmoid',   # use softmax if n_classes>1\n                 padding = 'same')(act9)\n\n    model = keras.Model(inputs=[inputs], outputs=[conv10])\n\n    return model","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-13T13:42:43.145894Z","iopub.execute_input":"2022-06-13T13:42:43.146289Z","iopub.status.idle":"2022-06-13T13:42:43.179258Z","shell.execute_reply.started":"2022-06-13T13:42:43.146262Z","shell.execute_reply":"2022-06-13T13:42:43.177071Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model summary**","metadata":{}},{"cell_type":"code","source":"# Model summary\nmodel=build_unet()\nmodel.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-13T13:42:43.183792Z","iopub.execute_input":"2022-06-13T13:42:43.186631Z","iopub.status.idle":"2022-06-13T13:42:43.531134Z","shell.execute_reply.started":"2022-06-13T13:42:43.18659Z","shell.execute_reply":"2022-06-13T13:42:43.530303Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot model architecture\nplot_model(model)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-13T13:42:43.532764Z","iopub.execute_input":"2022-06-13T13:42:43.533593Z","iopub.status.idle":"2022-06-13T13:42:43.937486Z","shell.execute_reply.started":"2022-06-13T13:42:43.533548Z","shell.execute_reply":"2022-06-13T13:42:43.936589Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Application: Tumor detection","metadata":{}},{"cell_type":"markdown","source":"**Get training files**","metadata":{}},{"cell_type":"code","source":"train_files = []\nmask_files = glob('../input/lgg-mri-segmentation/kaggle_3m/*/*_mask*')\n\nfor i in mask_files:\n    train_files.append(i.replace('_mask',''))","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:42:43.939402Z","iopub.execute_input":"2022-06-13T13:42:43.939986Z","iopub.status.idle":"2022-06-13T13:42:44.078196Z","shell.execute_reply.started":"2022-06-13T13:42:43.939942Z","shell.execute_reply":"2022-06-13T13:42:44.077331Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Preview some images**","metadata":{}},{"cell_type":"code","source":"#Lets plot some samples\nrows,cols=3,3\nfig=plt.figure(figsize=(10,10))\nfor i in range(1,rows*cols+1):\n    fig.add_subplot(rows,cols,i)\n    img_path=train_files[i]\n    msk_path=mask_files[i]\n    img=cv2.imread(img_path)\n    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    msk=cv2.imread(msk_path)\n    plt.imshow(img)\n    plt.imshow(msk,alpha=0.4)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:42:44.080589Z","iopub.execute_input":"2022-06-13T13:42:44.080967Z","iopub.status.idle":"2022-06-13T13:42:45.428483Z","shell.execute_reply.started":"2022-06-13T13:42:44.08093Z","shell.execute_reply":"2022-06-13T13:42:45.427707Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Split data**","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(data={\"filename\": train_files, 'mask' : mask_files})\ndf_train, df_test = train_test_split(df,test_size = 0.1)\ndf_train, df_val = train_test_split(df_train,test_size = 0.2)\nprint(df_train.values.shape)\nprint(df_val.values.shape)\nprint(df_test.values.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:42:45.4298Z","iopub.execute_input":"2022-06-13T13:42:45.430423Z","iopub.status.idle":"2022-06-13T13:42:45.442945Z","shell.execute_reply.started":"2022-06-13T13:42:45.430384Z","shell.execute_reply":"2022-06-13T13:42:45.441945Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Data generator**","metadata":{}},{"cell_type":"code","source":"# From: https://github.com/zhixuhao/unet/blob/master/data.py\ndef train_generator(data_frame, batch_size, aug_dict,\n        image_color_mode=\"rgb\",\n        mask_color_mode=\"grayscale\",\n        image_save_prefix=\"image\",\n        mask_save_prefix=\"mask\",\n        save_to_dir=None,\n        target_size=(256,256),\n        seed=1):\n    '''\n    can generate image and mask at the same time use the same seed for\n    image_datagen and mask_datagen to ensure the transformation for image\n    and mask is the same if you want to visualize the results of generator,\n    set save_to_dir = \"your path\"\n    '''\n    image_datagen = ImageDataGenerator(**aug_dict)\n    mask_datagen = ImageDataGenerator(**aug_dict)\n    \n    image_generator = image_datagen.flow_from_dataframe(\n        data_frame,\n        x_col = \"filename\",\n        class_mode = None,\n        color_mode = image_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = image_save_prefix,\n        seed = seed)\n\n    mask_generator = mask_datagen.flow_from_dataframe(\n        data_frame,\n        x_col = \"mask\",\n        class_mode = None,\n        color_mode = mask_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = mask_save_prefix,\n        seed = seed)\n\n    train_gen = zip(image_generator, mask_generator)\n    \n    for (img, mask) in train_gen:\n        img, mask = adjust_data(img, mask)\n        yield (img,mask)\n\ndef adjust_data(img,mask):\n    img = img / 255\n    mask = mask / 255\n    mask[mask > 0.5] = 1\n    mask[mask <= 0.5] = 0\n    \n    return (img, mask)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:42:45.444955Z","iopub.execute_input":"2022-06-13T13:42:45.445512Z","iopub.status.idle":"2022-06-13T13:42:45.458268Z","shell.execute_reply.started":"2022-06-13T13:42:45.445476Z","shell.execute_reply":"2022-06-13T13:42:45.457231Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Metrics**","metadata":{}},{"cell_type":"code","source":"smooth=100\n\ndef dice_coef(y_true, y_pred):\n    y_truef=K.flatten(y_true)\n    y_predf=K.flatten(y_pred)\n    And=K.sum(y_truef* y_predf)\n    return((2* And + smooth) / (K.sum(y_truef) + K.sum(y_predf) + smooth))\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1-dice_coef(y_true, y_pred)\n\ndef iou(y_true, y_pred):\n    intersection = K.sum(y_true * y_pred)\n    sum_ = K.sum(y_true + y_pred)\n    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n    return jac\n\ndef jac_distance(y_true, y_pred):\n    y_truef=K.flatten(y_true)\n    y_predf=K.flatten(y_pred)\n\n    return - iou(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:42:45.459195Z","iopub.execute_input":"2022-06-13T13:42:45.459472Z","iopub.status.idle":"2022-06-13T13:42:45.470993Z","shell.execute_reply.started":"2022-06-13T13:42:45.459448Z","shell.execute_reply":"2022-06-13T13:42:45.470179Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Config**","metadata":{}},{"cell_type":"code","source":"EPOCHS = 150\nBATCH_SIZE = 32\nlearning_rate = 1e-3\nim_width = 256\nim_height = 256","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:42:45.473902Z","iopub.execute_input":"2022-06-13T13:42:45.474283Z","iopub.status.idle":"2022-06-13T13:42:45.481777Z","shell.execute_reply.started":"2022-06-13T13:42:45.474258Z","shell.execute_reply":"2022-06-13T13:42:45.480833Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training**","metadata":{}},{"cell_type":"code","source":"train_generator_args = dict(rotation_range=0.2,\n                            width_shift_range=0.05,\n                            height_shift_range=0.05,\n                            shear_range=0.05,\n                            zoom_range=0.05,\n                            horizontal_flip=True,\n                            fill_mode='nearest')\n\ntrain_gen = train_generator(df_train, BATCH_SIZE,\n                                train_generator_args,\n                                target_size=(im_height, im_width))\n    \ntest_gener = train_generator(df_val, BATCH_SIZE,\n                                dict(),\n                                target_size=(im_height, im_width))\n\ndecay_rate = learning_rate / EPOCHS\nopt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate, amsgrad=False)\nmodel.compile(optimizer=opt, loss=dice_coef_loss, metrics=[\"binary_accuracy\", iou, dice_coef])\n\ncallbacks = [ModelCheckpoint('unet_brain_mri_seg.hdf5', verbose=1, save_best_only=True)]\n\nhistory = model.fit(train_gen,\n                    steps_per_epoch=len(df_train) / BATCH_SIZE, \n                    epochs=EPOCHS, \n                    #callbacks=callbacks,\n                    validation_data = test_gener,\n                    validation_steps=len(df_val) / BATCH_SIZE)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-13T13:42:45.48292Z","iopub.execute_input":"2022-06-13T13:42:45.483454Z","iopub.status.idle":"2022-06-13T13:51:05.15368Z","shell.execute_reply.started":"2022-06-13T13:42:45.483414Z","shell.execute_reply":"2022-06-13T13:51:05.152904Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Learning curves**","metadata":{}},{"cell_type":"code","source":"plt.figure(1)\nplt.plot(history.history['loss'],'b-', label='loss')\nplt.plot(history.history['val_loss'], 'r-', label='valid_loss')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss (1-Dice)', fontsize = 15)\nplt.legend()\n\nplt.figure(2)\nplt.plot(history.history['iou'], 'b-', label='iou')\nplt.plot(history.history['val_iou'], 'r-', label='valid_iou')\nplt.xlabel('Iteration')\nplt.ylabel('IoU')\nplt.title('IoU', fontsize = 15)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:51:05.158977Z","iopub.execute_input":"2022-06-13T13:51:05.161067Z","iopub.status.idle":"2022-06-13T13:51:05.665475Z","shell.execute_reply.started":"2022-06-13T13:51:05.161026Z","shell.execute_reply":"2022-06-13T13:51:05.664725Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluate**","metadata":{}},{"cell_type":"code","source":"test_gen = train_generator(df_test, BATCH_SIZE,\n                                dict(),\n                                target_size=(im_height, im_width))\n\nresults = model.evaluate(test_gen, steps=len(df_test) / BATCH_SIZE)\n\nprint(\"Test loss: \",results[0])\nprint(\"Test IOU: \",results[1])\nprint(\"Test Dice Coefficent: \",results[2])","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:51:05.673016Z","iopub.execute_input":"2022-06-13T13:51:05.673929Z","iopub.status.idle":"2022-06-13T13:51:14.951889Z","shell.execute_reply.started":"2022-06-13T13:51:05.673891Z","shell.execute_reply":"2022-06-13T13:51:14.950911Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Plot predictions**","metadata":{}},{"cell_type":"code","source":"for i in range(20):\n    index=np.random.randint(1,len(df_test.index))\n    img = cv2.imread(df_test['filename'].iloc[index])\n    img = cv2.resize(img ,(im_height, im_width))\n    img = img / 255\n    img = img[np.newaxis, :, :, :]\n    pred=model.predict(img)\n\n    plt.figure(figsize=(12,12))\n    plt.subplot(1,3,1)\n    plt.imshow(np.squeeze(img))\n    plt.title('Original Image')\n    plt.subplot(1,3,2)\n    plt.imshow(np.squeeze(cv2.imread(df_test['mask'].iloc[index])))\n    plt.title('Original Mask')\n    plt.subplot(1,3,3)\n    plt.imshow(np.squeeze(pred) > .5)\n    plt.title('Prediction')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T13:51:14.953137Z","iopub.execute_input":"2022-06-13T13:51:14.953754Z","iopub.status.idle":"2022-06-13T13:51:26.017583Z","shell.execute_reply.started":"2022-06-13T13:51:14.953712Z","shell.execute_reply":"2022-06-13T13:51:26.016834Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Acknowledgments:**\n* [Brain MRI Segmentation| Using Unet | Keras](https://www.kaggle.com/code/monkira/brain-mri-segmentation-using-unet-keras) by [MonKira](https://www.kaggle.com/monkira).\n* [Convolutional Neural Networks](https://www.coursera.org/learn/convolutional-neural-networks) by [Andrew NG](https://en.wikipedia.org/wiki/Andrew_Ng).","metadata":{}},{"cell_type":"markdown","source":"Thanks for reading!","metadata":{}}]}